{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 500px Machine Learning Engineer Intern - Tech Challenge\n",
    "## Fooling an MNIST Classifier with Adversarial Images, using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Create adversarial images to fool a MNIST classifier in TensorFlow.\n",
    "\n",
    "Deep convolutional neural networks (CNN) are state of the art models for image classification and object detection. Such models play crucial role at 500px where we use them for many applications like automatic keywording, people detection and image search. It’s important to understand how they work and what their limitations are.\n",
    "One known “limitation” of CNN is that they can be fooled to misclassify an image with high confidence by slightly\n",
    "perturbing the pixels. \n",
    "\n",
    "This is illustrated on the image below:\n",
    "\n",
    "![](https://lh4.googleusercontent.com/Bz7CFzzMBRkKJ4xGqMTpufuL35Lf69z3DEoDAV-ZzD1OC9lMHYL4co0ED-LF2URMowvbDdqkRg6oxZHWeIspOVDkeaB0rqAfNpRHXfrhxS45U2cqsuX52J2GZwlFOB0TSc_rYxu7)\n",
    "\n",
    "The delta between the original image and the adversarial one is so small that it is impossible for humans to detect. The fun fact is other machine learning models like SVM and logistic regression can be tricked in the similar manner.\n",
    "\n",
    "Note that the “fast gradient sign” method presented in the [original paper by Goodfellow](https://arxiv.org/abs/1412.6572) produces adversarial images for a random target class. In this challenge we would like to generate adversarial images to misclassify any examples of ‘2’ as ‘6’ specifically. This puts certain implications on the final solution.\n",
    "\n",
    "One of the useful application for adversarial images is that if you train your deep CNN classifier on them you can improve its accuracy on non-adversarial examples.\n",
    "\n",
    "In this challenge you are given an opportunity to learn how to generate adversarial examples and also gain practical experience using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Getting Started\n",
    "\n",
    "Let's start by importing dependencies, and loading up the MNIST dataset. Note that much of the code in this section, and sections to come, is repurposed from [this tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/#deep-mnist-for-experts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Neural Network \n",
    "\n",
    "Now that we have the data, let's get our feet wet with TensorFlow by implementing a basic fully connected neural network with one hidden layer, and see how it performs on the data. Note that aside from using the TensorFlow code mentioned earlier, much of this code is repurposed from an earlier project I've completed, which can be found [here](https://github.com/MunirAD/Facial_Recognition_AlexNet/blob/master/part2.py) (in the `fully_connected` function code).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9154\n"
     ]
    }
   ],
   "source": [
    "def FullyConnectedNet(num_in, num_hid, num_out, lam, learn_rate, num_epochs, batch_size):\n",
    "    \n",
    "    # Set up a placeholder for the input 'x' and its label 'y_' \n",
    "    # Note the label is in one-of-k (one-hot) encoding\n",
    "    x = tf.placeholder(tf.float32, [None, num_in])\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_out])\n",
    "\n",
    "    # Set up variables for the network parameters. Note this is a single-hidden\n",
    "    # layer network\n",
    "    W0 = tf.Variable(tf.random_normal([num_in, num_hid], stddev=0.01))\n",
    "    b0 = tf.Variable(tf.random_normal([num_hid], stddev=0.01))\n",
    "    W1 = tf.Variable(tf.random_normal([num_hid, num_out], stddev=0.01))\n",
    "    b1 = tf.Variable(tf.random_normal([num_out], stddev=0.01))\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # Set up the computation of a forward pass on an input to the net\n",
    "    layer1 = tf.nn.relu(tf.matmul(x, W0) + b0)\n",
    "    layer2 = tf.matmul(layer1, W1) + b1\n",
    "    y = tf.nn.softmax(layer2)\n",
    "    \n",
    "    # Set up a decay penalty to regularize, reducing the risk of over-fitting\n",
    "    decay_penalty = lam*tf.reduce_sum(tf.square(W0)) + lam*tf.reduce_sum(tf.square(W1))\n",
    "    NLL = -tf.reduce_sum(y_*tf.log(y)) + decay_penalty\n",
    "    \n",
    "    # Set up the Gradient Descent optimization step on the objective function \n",
    "    # with the given learning rate\n",
    "    train_step = tf.train.GradientDescentOptimizer(learn_rate).minimize(NLL)\n",
    "    \n",
    "    # Set up the logic for what a correct prediction is, and \n",
    "    # classification accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Do mini batch gradient descent\n",
    "    for i in range(num_epochs):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        train_step.run(feed_dict = {x: batch[0], y_ : batch[1]})\n",
    "    \n",
    "    # Print the accuracy on the validation data\n",
    "    print(accuracy.eval(feed_dict={x: mnist.validation.images, y_: mnist.validation.labels}))\n",
    "\n",
    "FullyConnectedNet(784, 300, 10, 0.01, 0.0005, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
